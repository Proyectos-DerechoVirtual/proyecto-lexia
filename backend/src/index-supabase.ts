import express from 'express';
import cors from 'cors';
import helmet from 'helmet';
import dotenv from 'dotenv';
import { createClient } from '@supabase/supabase-js';
import OpenAI from 'openai';
import nodemailer from 'nodemailer';
import { logger } from './utils/logger';
import { getUnifiedRAGService } from './services/ragServiceUnified';
import * as geminiRag from './services/geminiRagService';

// Cargar variables de entorno
dotenv.config(); // Carga .env desde el directorio actual del backend
dotenv.config({ path: '../.env' }); // Tambi√©n intenta cargar desde el directorio padre

console.log('PORT from env:', process.env.PORT); // Debug

const app = express();
const PORT = process.env.PORT || 4000;

// Configurar Supabase
const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

// Funci√≥n para crear cliente Supabase con token de usuario
const getUserSupabaseClient = (userToken: string) => {
  return createClient(
    process.env.SUPABASE_URL!,
    process.env.SUPABASE_ANON_KEY!,
    {
      global: {
        headers: {
          Authorization: `Bearer ${userToken}`
        }
      }
    }
  );
};

// Configurar OpenAI
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

// Inicializar RAG Service unificado
const ragService = getUnifiedRAGService();

// Pre-calentar cache al iniciar
ragService.warmupCache().catch(err => 
  logger.error('Error pre-calentando cache:', err)
);

// Middleware
app.use(helmet());

// Configuraci√≥n de CORS para producci√≥n y desarrollo
const allowedOrigins = process.env.NODE_ENV === 'production'
  ? [
      process.env.FRONTEND_URL,
      'https://lexia-chatbot.vercel.app',
      'https://lexia.vercel.app',
      // Permitir Teachable espec√≠fico
      'https://derechovirtual.teachable.com',
      // Patrones wildcard para otras plataformas
      'https://*.teachable.com',
      'https://*.thinkific.com',
      'https://*.kajabi.com',
      'https://*.podia.com'
    ].filter(Boolean)
  : ['http://localhost:3000', 'http://localhost:4000', 'http://localhost:3001', 'http://localhost:3002'];

app.use(cors({
  origin: (origin, callback) => {
    // Permitir requests sin origin (ej: Postman, server-side, webhooks)
    if (!origin) return callback(null, true);

    // Verificar si el origin est√° en la lista exacta
    if (allowedOrigins.includes(origin)) {
      return callback(null, true);
    }

    // Verificar si el origin coincide con alg√∫n patr√≥n wildcard
    const isAllowedPattern = allowedOrigins.some(pattern => {
      if (pattern && pattern.includes('*')) {
        // Escapar puntos primero, luego reemplazar * con .*
        const escaped = pattern.replace(/\./g, '\\.').replace(/\*/g, '.*');
        const regex = new RegExp(`^${escaped}$`);
        return regex.test(origin);
      }
      return false;
    });

    if (isAllowedPattern) {
      callback(null, true);
    } else {
      // En producci√≥n, loggear origins rechazados para debug
      console.log('CORS rechazado para origin:', origin);
      callback(new Error('Not allowed by CORS'));
    }
  },
  credentials: true,
  optionsSuccessStatus: 200
}));

app.use(express.json());

// Agregar compresi√≥n para producci√≥n (opcional, comentado por ahora)
// if (process.env.NODE_ENV === 'production') {
//   const compression = require('compression');
//   app.use(compression());
// }

// Extend Express Request type
declare global {
  namespace Express {
    interface Request {
      user?: any;
      userToken?: string;
    }
  }
}

// Middleware de autenticaci√≥n
const authenticateToken = async (req: any, res: any, next: any) => {
  const authHeader = req.headers['authorization'];
  const token = authHeader && authHeader.split(' ')[1];

  if (!token) {
    return res.status(401).json({ error: 'Token requerido' });
  }

  try {
    const { data: { user }, error } = await supabase.auth.getUser(token);
    if (error || !user) {
      return res.status(403).json({ error: 'Token inv√°lido' });
    }
    req.user = user;
    req.userToken = token; // Guardar el token para usar en operaciones de DB
    next();
  } catch (error) {
    return res.status(403).json({ error: 'Error de autenticaci√≥n' });
  }
};

// Determinar si usar Gemini o el sistema antiguo
const USE_GEMINI = !!process.env.GEMINI_FILE_STORE_NAME;
logger.info(`ü§ñ RAG Mode: ${USE_GEMINI ? 'Gemini File Search' : 'Supabase + OpenAI'}`);

// Health check
app.get('/health', (_req, res) => {
  res.json({
    status: 'ok',
    timestamp: new Date().toISOString(),
    mode: 'supabase-backend',
    ragMode: USE_GEMINI ? 'gemini' : 'supabase',
    version: '1.1.0'
  });
});

// Verificar configuraci√≥n de Gemini
app.get('/api/gemini/status', async (_req, res) => {
  try {
    const status = await geminiRag.verifyGeminiConfig();
    res.json(status);
  } catch (error: any) {
    res.status(500).json({ ok: false, message: error.message });
  }
});

// Test endpoint para verificar OpenAI
app.get('/test-openai', async (_req, res) => {
  try {
    const startTime = Date.now();
    logger.info('üß™ Iniciando test de OpenAI...');
    
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: 'Responde en una sola l√≠nea.' },
        { role: 'user', content: '¬øCu√°l es la capital de Espa√±a?' }
      ],
      max_tokens: 50,
      temperature: 0.7
    });
    
    const elapsed = Date.now() - startTime;
    
    res.json({
      success: true,
      responseTime: elapsed,
      response: completion.choices[0].message.content,
      model: completion.model
    });
  } catch (error: any) {
    logger.error('Error en test de OpenAI:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// Auth routes
app.post('/api/auth/login', async (req, res) => {
  try {
    const { email, password } = req.body;
    const { data, error } = await supabase.auth.signInWithPassword({
      email,
      password,
    });

    if (error) {
      res.status(401).json({ error: error.message });
      return;
    }

    res.json({
      user: data.user,
      token: data.session?.access_token,
    });
  } catch (error: any) {
    logger.error('Login error:', error);
    res.status(500).json({ error: 'Error de servidor' });
  }
});

app.post('/api/auth/register', async (req, res) => {
  try {
    const { name, email, password } = req.body;
    const { data, error } = await supabase.auth.signUp({
      email,
      password,
      options: {
        data: { name }
      }
    });

    if (error) {
      res.status(400).json({ error: error.message });
      return;
    }

    res.json({
      user: data.user,
      token: data.session?.access_token,
    });
  } catch (error: any) {
    logger.error('Register error:', error);
    res.status(500).json({ error: 'Error de servidor' });
  }
});

app.get('/api/auth/me', authenticateToken, async (req, res) => {
  res.json({ user: req.user });
});

// Conversation routes
app.get('/api/conversations', authenticateToken, async (req, res) => {
  try {
    const userId = req.user.id;
    const userSupabase = getUserSupabaseClient(req.userToken);
    
    const { data, error } = await userSupabase
      .from('conversations')
      .select('*')
      .eq('user_id', userId)
      .order('updated_at', { ascending: false });

    if (error) {
      res.status(500).json({ error: error.message });
      return;
    }

    res.json({ data });
  } catch (error: any) {
    logger.error('Get conversations error:', error);
    res.status(500).json({ error: 'Error de servidor' });
  }
});

app.post('/api/conversations', authenticateToken, async (req, res) => {
  try {
    const { title, category } = req.body;
    const userId = req.user.id;
    const userSupabase = getUserSupabaseClient(req.userToken);

    const { data, error } = await userSupabase
      .from('conversations')
      .insert({
        title: title || 'Nueva consulta',
        category: category || 'otro',
        user_id: userId,
      })
      .select()
      .single();

    if (error) {
      res.status(500).json({ error: error.message });
      return;
    }

    res.json({ data });
  } catch (error: any) {
    logger.error('Create conversation error:', error);
    res.status(500).json({ error: 'Error de servidor' });
  }
});

app.get('/api/conversations/:id', authenticateToken, async (req, res) => {
  try {
    const { id } = req.params;
    const userId = req.user.id;
    const userSupabase = getUserSupabaseClient(req.userToken);

    // Get conversation
    const { data: conversation, error: convError } = await userSupabase
      .from('conversations')
      .select('*')
      .eq('id', id)
      .eq('user_id', userId)
      .single();

    if (convError || !conversation) {
      return res.status(404).json({ error: 'Conversaci√≥n no encontrada' });
    }

    // Get messages
    const { data: messages, error: msgError } = await userSupabase
      .from('messages')
      .select('*')
      .eq('conversation_id', id)
      .order('created_at', { ascending: true });

    if (msgError) {
      return res.status(500).json({ error: msgError.message });
    }

    res.json({ data: { conversation, messages } });
  } catch (error: any) {
    logger.error('Get conversation error:', error);
    res.status(500).json({ error: 'Error de servidor' });
  }
});

app.put('/api/conversations/:id/title', authenticateToken, async (req, res) => {
  try {
    const { id } = req.params;
    const { title } = req.body;
    const userId = req.user.id;
    const userSupabase = getUserSupabaseClient(req.userToken);

    const { error } = await userSupabase
      .from('conversations')
      .update({ title })
      .eq('id', id)
      .eq('user_id', userId);

    if (error) {
      res.status(500).json({ error: error.message });
      return;
    }

    res.json({ success: true });
  } catch (error: any) {
    logger.error('Update conversation title error:', error);
    res.status(500).json({ error: 'Error de servidor' });
  }
});

app.delete('/api/conversations/:id', authenticateToken, async (req, res) => {
  try {
    const { id } = req.params;
    const userId = req.user.id;
    const userSupabase = getUserSupabaseClient(req.userToken);

    const { error } = await userSupabase
      .from('conversations')
      .delete()
      .eq('id', id)
      .eq('user_id', userId);

    if (error) {
      res.status(500).json({ error: error.message });
      return;
    }

    res.json({ success: true });
  } catch (error: any) {
    logger.error('Delete conversation error:', error);
    res.status(500).json({ error: 'Error de servidor' });
  }
});

// Message routes - Versi√≥n simple sin RAG para testing
app.post('/api/messages-simple', authenticateToken, async (req, res) => {
  try {
    const startTime = Date.now();
    const { conversationId, content } = req.body;
    const userId = req.user.id;
    const userSupabase = getUserSupabaseClient(req.userToken);

    // Solo verificar conversaci√≥n y guardar mensaje
    const { data: conversation } = await userSupabase
      .from('conversations')
      .select('*')
      .eq('id', conversationId)
      .eq('user_id', userId)
      .single();

    if (!conversation) {
      return res.status(404).json({ error: 'Conversaci√≥n no encontrada' });
    }

    // Guardar mensaje del usuario
    const { data: userMessage } = await userSupabase
      .from('messages')
      .insert({
        conversation_id: conversationId,
        content,
        role: 'user'
      })
      .select()
      .single();

    // Llamada directa a OpenAI sin RAG ni contexto
    logger.info('üöÄ Llamada simple a OpenAI...');
    const aiStart = Date.now();
    
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { 
          role: 'system', 
          content: getLegalSystemPrompt()
        },
        { role: 'user', content }
      ],
      max_tokens: 3500, // Consistente con el endpoint principal
      temperature: 0.7
    });
    
    const aiTime = Date.now() - aiStart;
    logger.info(`‚úÖ OpenAI simple respondi√≥ en ${aiTime}ms`);

    // Guardar respuesta
    const { data: assistantMessage } = await userSupabase
      .from('messages')
      .insert({
        conversation_id: conversationId,
        content: completion.choices[0].message.content,
        role: 'assistant'
      })
      .select()
      .single();

    const totalTime = Date.now() - startTime;
    logger.info(`‚úÖ Tiempo total (sin RAG): ${totalTime}ms`);

    res.json({
      data: {
        userMessage,
        assistantMessage,
        metrics: { totalTime, aiTime }
      }
    });
  } catch (error: any) {
    logger.error('Error en mensaje simple:', error);
    res.status(500).json({ error: error.message });
  }
});

// Message routes
// Endpoint con streaming para mejor UX
app.post('/api/messages-stream', authenticateToken, async (req, res) => {
  const { conversationId, content } = req.body;
  const userId = req.user.id;
  const userSupabase = getUserSupabaseClient(req.userToken);

  // Configurar Server-Sent Events
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  res.setHeader('Access-Control-Allow-Origin', '*');

  const sendEvent = (type: string, data: any) => {
    res.write(`data: ${JSON.stringify({ type, data })}\n\n`);
  };

  try {
    const totalStartTime = Date.now();

    // Verificar conversaci√≥n y guardar mensaje del usuario
    const { data: conversation, error: convError } = await userSupabase
      .from('conversations')
      .select('*')
      .eq('id', conversationId)
      .eq('user_id', userId)
      .single();

    if (convError || !conversation) {
      sendEvent('error', { message: 'Conversaci√≥n no encontrada' });
      return res.end();
    }

    const { data: userMessage } = await userSupabase
      .from('messages')
      .insert({
        conversation_id: conversationId,
        content,
        role: 'user'
      })
      .select()
      .single();

    sendEvent('user_message', userMessage);

    // Obtener historial
    const { data: recentMessages } = await userSupabase
      .from('messages')
      .select('content, role')
      .eq('conversation_id', conversationId)
      .order('created_at', { ascending: false })
      .limit(15);

    const conversationHistory = recentMessages?.reverse().map(msg => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content,
    })) || [];

    let fullResponse = '';

    // ========== USAR GEMINI FILE SEARCH ==========
    if (USE_GEMINI) {
      sendEvent('status', { message: 'Buscando en documentos...', step: 1, total: 2, estimated: '~2s' });
      sendEvent('status', { message: 'Generando respuesta con IA...', step: 2, total: 2, estimated: '~4s' });

      const result = await geminiRag.generateResponseWithRAGStream(
        content,
        conversationHistory,
        (chunk) => {
          fullResponse += chunk;
          sendEvent('content', { content: chunk, fullContent: fullResponse });
        }
      );

      fullResponse = result.content;
      logger.info(`‚úÖ Gemini File Search completado`);
    }
    // ========== USAR SISTEMA ANTIGUO (OpenAI + Supabase) ==========
    else {
      // PASO 1/3: AN√ÅLISIS Y CLASIFICACI√ìN DE LA PREGUNTA
      sendEvent('status', { message: 'Analizando pregunta...', step: 1, total: 3, estimated: '~2s' });

      // PASO 2/3: B√öSQUEDA RAG H√çBRIDA AVANZADA
      sendEvent('status', { message: 'Buscando informaci√≥n relevante...', step: 2, total: 3, estimated: '~2s' });

      // Usar el pipeline mejorado de 3 pasos
      const { analysis, refinedContext, mainResponse } = await enhancedLegalResponse(
        content,
        '', // Se obtendr√° contexto refinado en el pipeline
        conversationHistory,
        conversation.category
      );

      // PASO 3/3: GENERACI√ìN DE RESPUESTA ESPECIALIZADA CON STREAMING
      sendEvent('status', { message: 'Generando respuesta especializada...', step: 3, total: 3, estimated: '~6s' });

      for await (const chunk of mainResponse) {
        const chunkContent = chunk.choices[0]?.delta?.content || '';
        if (chunkContent) {
          fullResponse += chunkContent;
          sendEvent('content', { content: chunkContent, fullContent: fullResponse });
        }
      }

      logger.info(`‚úÖ Pipeline OpenAI completado`);
    }

    // Guardar respuesta completa
    const { data: assistantMessage } = await userSupabase
      .from('messages')
      .insert({
        conversation_id: conversationId,
        content: fullResponse,
        role: 'assistant'
      })
      .select()
      .single();

    // Generar t√≠tulo autom√°ticamente si es el primer mensaje
    const { data: messageCount } = await userSupabase
      .from('messages')
      .select('id', { count: 'exact' })
      .eq('conversation_id', conversationId);

    if (messageCount && messageCount.length <= 2) {
      // Usar Gemini o OpenAI para generar t√≠tulo
      const newTitle = USE_GEMINI
        ? await geminiRag.generateConversationTitle(content)
        : await generateConversationTitle(content);

      await userSupabase
        .from('conversations')
        .update({ title: newTitle })
        .eq('id', conversationId);

      sendEvent('title_updated', { conversationId, title: newTitle });
    }

    sendEvent('complete', {
      assistantMessage,
      totalTime: Date.now() - totalStartTime
    });

    res.end();

  } catch (error: any) {
    logger.error('Stream error:', error);
    sendEvent('error', { message: error.message });
    res.end();
  }
});

app.post('/api/messages', authenticateToken, async (req, res) => {
  try {
    const totalStartTime = Date.now();
    const { conversationId, content } = req.body;
    const userId = req.user.id;
    const userSupabase = getUserSupabaseClient(req.userToken);

    // Verificar que la conversaci√≥n pertenece al usuario
    const { data: conversation, error: convError } = await userSupabase
      .from('conversations')
      .select('*')
      .eq('id', conversationId)
      .eq('user_id', userId)
      .single();

    if (convError || !conversation) {
      return res.status(404).json({ error: 'Conversaci√≥n no encontrada' });
    }

    // Guardar mensaje del usuario
    const { data: userMessage, error: userMsgError } = await userSupabase
      .from('messages')
      .insert({
        conversation_id: conversationId,
        content,
        role: 'user'
      })
      .select()
      .single();

    if (userMsgError) {
      return res.status(500).json({ error: userMsgError.message });
    }

    // Obtener historial de mensajes recientes para contexto
    const { data: recentMessages } = await userSupabase
      .from('messages')
      .select('content, role')
      .eq('conversation_id', conversationId)
      .order('created_at', { ascending: false })
      .limit(15);

    // Obtener contexto enriquecido con RAG
    const ragContext = await ragService.getEnhancedContext(content, conversation.category);
    
    // Construir el contexto del chat
    const systemPrompt = getLegalSystemPrompt(conversation.category);
    const enhancedSystemPrompt = ragContext 
      ? `${systemPrompt}\n\n=== INFORMACI√ìN ESPEC√çFICA DE LA BASE DE DATOS ===\n${ragContext}\n=== FIN DE LA INFORMACI√ìN ESPEC√çFICA ===`
      : systemPrompt;

    const messages = [
      {
        role: 'system' as const,
        content: enhancedSystemPrompt,
      },
      ...recentMessages?.reverse().map(msg => ({
        role: msg.role as 'user' | 'assistant',
        content: msg.content,
      })) || [],
    ];

    // Generar respuesta con OpenAI
    let responseContent = 'No se pudo generar una respuesta.';
    
    try {
      const completion = await Promise.race([
        openai.chat.completions.create({
          model: 'gpt-4o', // Forzar modelo m√°s r√°pido
          messages,
          max_tokens: 3500, // Aumentado para respuestas m√°s completas con an√°lisis detallado
          temperature: 0.7,
          stream: false,
          // Agregar configuraci√≥n adicional para optimizaci√≥n
          presence_penalty: 0,
          frequency_penalty: 0,
          top_p: 1,
          n: 1
        }),
        new Promise((_, reject) => 
          setTimeout(() => reject(new Error('OpenAI timeout')), 15000) // Aumentado a 15s para dar m√°s margen
        )
      ]) as any;

      responseContent = completion.choices[0].message.content || 'No se pudo generar una respuesta.';
      
    } catch (openAIError: any) {
      logger.error(`‚ùå Error de OpenAI:`, openAIError.message);
      throw openAIError;
    }

    // Guardar respuesta del asistente
    const { data: assistantMessage, error: assistantMsgError } = await userSupabase
      .from('messages')
      .insert({
        conversation_id: conversationId,
        content: responseContent,
        role: 'assistant'
      })
      .select()
      .single();

    if (assistantMsgError) {
      return res.status(500).json({ error: assistantMsgError.message });
    }

    // Actualizar timestamp de conversaci√≥n
    await userSupabase
      .from('conversations')
      .update({ updated_at: new Date().toISOString() })
      .eq('id', conversationId);

    const totalTime = Date.now() - totalStartTime;
    logger.info(`‚úÖ Tiempo total de respuesta: ${totalTime}ms`);

    res.json({
      data: {
        userMessage,
        assistantMessage,
        totalTime
      }
    });
  } catch (error: any) {
    logger.error('Send message error:', error);
    
    // Dar m√°s informaci√≥n sobre el tipo de error
    let errorMessage = 'Error al generar respuesta';
    if (error.message === 'OpenAI timeout') {
      errorMessage = 'La IA tard√≥ demasiado en responder. Intenta de nuevo.';
    } else if (error.response?.status === 401) {
      errorMessage = 'Error de autenticaci√≥n con OpenAI';
    } else if (error.response?.status === 429) {
      errorMessage = 'L√≠mite de uso de OpenAI alcanzado';
    } else if (error.code === 'insufficient_quota') {
      errorMessage = 'Cuota de OpenAI agotada';
    }
    
    res.status(500).json({ 
      error: errorMessage,
      details: process.env.NODE_ENV === 'development' ? error.message : undefined
    });
  }
});

app.get('/api/messages/search', authenticateToken, async (req, res) => {
  try {
    const { q: query, conversationId } = req.query;
    const userId = req.user.id;

    let searchQuery = supabase
      .from('messages')
      .select(`
        *,
        conversations!inner(user_id)
      `)
      .eq('conversations.user_id', userId)
      .ilike('content', `%${query}%`);

    if (conversationId) {
      searchQuery = searchQuery.eq('conversation_id', conversationId);
    }

    const { data, error } = await searchQuery.order('created_at', { ascending: false });

    if (error) {
      res.status(500).json({ error: error.message });
      return;
    }

    res.json({ data });
  } catch (error: any) {
    logger.error('Search messages error:', error);
    res.status(500).json({ error: 'Error de servidor' });
  }
});

// Endpoint para generar respuestas del chat (legacy)
app.post('/api/chat/generate', authenticateToken, async (req, res) => {
  try {
    const { conversationId, message, category } = req.body;
    const userId = req.user.id;

    // Verificar que la conversaci√≥n pertenece al usuario
    const { data: conversation, error: convError } = await supabase
      .from('conversations')
      .select('*')
      .eq('id', conversationId)
      .eq('user_id', userId)
      .single();

    if (convError || !conversation) {
      return res.status(404).json({ error: 'Conversaci√≥n no encontrada' });
    }

    // Obtener historial de mensajes recientes para contexto
    const { data: recentMessages } = await supabase
      .from('messages')
      .select('content, role')
      .eq('conversation_id', conversationId)
      .order('created_at', { ascending: false })
      .limit(15);

    // Construir el contexto del chat
    const messages = [
      {
        role: 'system' as const,
        content: getLegalSystemPrompt(category),
      },
      ...recentMessages?.reverse().map(msg => ({
        role: msg.role as 'user' | 'assistant',
        content: msg.content,
      })) || [],
      {
        role: 'user' as const,
        content: message,
      },
    ];

    // Generar respuesta con OpenAI con timeout
    const completion = await Promise.race([
      openai.chat.completions.create({
        model: process.env.OPENAI_MODEL || 'gpt-4o',
        messages,
        max_tokens: 1500,
        temperature: 0.7,
      }),
      new Promise((_, reject) => 
        setTimeout(() => reject(new Error('OpenAI timeout')), 25000)
      )
    ]) as any;

    const responseContent = completion.choices[0].message.content || 'No se pudo generar una respuesta.';

    // Generar t√≠tulo autom√°ticamente si es el primer mensaje
    const { data: messageCount } = await supabase
      .from('messages')
      .select('id', { count: 'exact' })
      .eq('conversation_id', conversationId);

    if (messageCount && messageCount.length <= 2) { // Usuario + asistente = primer intercambio
      const newTitle = await generateConversationTitle(message);
      await supabase
        .from('conversations')
        .update({ title: newTitle })
        .eq('id', conversationId);
    }

    res.json({
      content: responseContent,
      model: completion.model,
      tokens: completion.usage?.total_tokens,
    });
  } catch (error: any) {
    logger.error('Error generando respuesta:', error);
    
    // Dar m√°s informaci√≥n sobre el tipo de error
    let errorMessage = 'Error al generar respuesta';
    if (error.message === 'OpenAI timeout') {
      errorMessage = 'La IA tard√≥ demasiado en responder. Intenta de nuevo.';
    } else if (error.response?.status === 401) {
      errorMessage = 'Error de autenticaci√≥n con OpenAI';
    } else if (error.response?.status === 429) {
      errorMessage = 'L√≠mite de uso de OpenAI alcanzado';
    } else if (error.code === 'insufficient_quota') {
      errorMessage = 'Cuota de OpenAI agotada';
    }
    
    res.status(500).json({ 
      error: errorMessage,
      details: process.env.NODE_ENV === 'development' ? error.message : undefined
    });
  }
});

// RAG endpoints
app.post('/api/rag/process-documents', authenticateToken, async (req, res) => {
  try {
    // Solo permitir a administradores (puedes ajustar esta l√≥gica)
    if (!req.user.email?.includes('admin')) {
      return res.status(403).json({ error: 'Solo administradores pueden procesar documentos' });
    }

    // Este endpoint no est√° implementado en el servicio unificado
    // ya que los documentos ya est√°n procesados en Supabase
    res.json({ 
      message: 'Los documentos ya est√°n procesados en la base de datos',
      info: 'Use el endpoint /api/rag/stats para ver estad√≠sticas'
    });
  } catch (error: any) {
    logger.error('Error procesando documentos:', error);
    res.status(500).json({ error: 'Error procesando documentos' });
  }
});

app.get('/api/rag/stats', authenticateToken, async (_req, res) => {
  try {
    const stats = await ragService.getDocumentStats();
    res.json({ data: stats });
  } catch (error: any) {
    logger.error('Error obteniendo estad√≠sticas:', error);
    res.status(500).json({ error: 'Error obteniendo estad√≠sticas' });
  }
});

app.post('/api/rag/search', authenticateToken, async (req, res) => {
  try {
    const { query, threshold = 0.8, limit = 5 } = req.body;
    
    if (!query) {
      return res.status(400).json({ error: 'Query requerido' });
    }

    const results = await ragService.searchRelevantDocuments(query, threshold, limit);
    res.json({ data: results });
  } catch (error: any) {
    logger.error('Error en b√∫squeda RAG:', error);
    res.status(500).json({ error: 'Error en b√∫squeda' });
  }
});

// Funci√≥n para generar t√≠tulos de conversaci√≥n
async function generateConversationTitle(firstMessage: string): Promise<string> {
  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'Genera un t√≠tulo corto y descriptivo (m√°ximo 6 palabras) para una conversaci√≥n sobre derecho que empez√≥ con este mensaje. Solo devuelve el t√≠tulo, nada m√°s.'
        },
        {
          role: 'user',
          content: firstMessage
        }
      ],
      max_tokens: 50,
      temperature: 0.3
    });

    return completion.choices[0].message.content?.trim() || 'Nueva consulta legal';
  } catch (error) {
    logger.error('Error generando t√≠tulo:', error);
    return 'Nueva consulta legal';
  }
}

// Pipeline de llamadas especializadas para mayor precisi√≥n
async function enhancedLegalResponse(userQuestion: string, context: string, conversationHistory: any[], category?: string) {
  try {
    logger.info('üîÑ Iniciando pipeline de respuesta mejorada...');
    
    // LLAMADA 1: An√°lisis y clasificaci√≥n de la pregunta
    logger.info('üìä Paso 1/4: Analizando y clasificando la pregunta...');
    
    const analysis = await analyzeUserQuestion(userQuestion);
    logger.info(`‚úÖ An√°lisis: ${analysis.tipo}, complejidad: ${analysis.complejidad}`);

    // LLAMADA 2: Refinamiento de contexto RAG
    logger.info('üîç Paso 2/4: Refinando b√∫squeda con an√°lisis...');
    const refinedContext = await ragService.getEnhancedContextWithKeywords(
      userQuestion, 
      analysis.keywords_busqueda || [], 
      category,
      analysis
    );

    // LLAMADA 3: Generaci√≥n de respuesta especializada
    logger.info('‚ö° Paso 3/3: Generando respuesta especializada...');
    const responsePrompt = getSpecializedPrompt(analysis.tipo, category);
    const fullContext = refinedContext || context;
    
    
    const enhancedPrompt = fullContext 
      ? `${responsePrompt}\n\n=== INFORMACI√ìN ESPEC√çFICA DE LA BASE DE DATOS ===\n${fullContext}\n=== FIN DE LA INFORMACI√ìN ESPEC√çFICA ===`
      : responsePrompt;

    const messages = [
      { role: 'system' as const, content: enhancedPrompt },
      ...conversationHistory,
      { role: 'user' as const, content: userQuestion }
    ];

    const mainResponse = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages,
      max_tokens: 4000,
      temperature: 0.3,
      stream: true
    });

    // LLAMADA 4: Generaci√≥n de modo estudio contextual (ser√° llamada despu√©s)
    return { analysis, refinedContext: fullContext, mainResponse };

  } catch (error) {
    logger.error('Error en pipeline mejorado:', error);
    throw error;
  }
}

// Funci√≥n para generar modo estudio basado en la respuesta anterior

// Funci√≥n unificada para obtener el prompt legal (elimina duplicaci√≥n)
function getUnifiedLegalPrompt(includeAvailableSources: boolean = false): string {
  const basePrompt = `Eres LexAI, un asistente legal especializado en oposiciones de justicia con el estilo de Carlos Rivero (Derecho Virtual). Tienes acceso a leyes espa√±olas y materiales de clase. Tu forma de hablar es coloquial, directa, amigable y retadora, como un compa√±ero que domina la materia.

**INSTRUCCIONES PARA RESPONDER:**

1. **ANALIZA EL CONTEXTO:** Revisa cuidadosamente los chunks encontrados y el historial de conversaci√≥n para entender exactamente qu√© te est√° preguntando el usuario.

2. **ESTILO CARLOS RIVERO - COLEGUEO Y RIGOR:** 
   - **INICIO POSITIVO COLOQUIAL:** Comienza con expresiones naturales como "¬°Vale, perfecto!", "¬°Mira, genial pregunta!", "¬°A ver, esto me encanta!", "¬°Oye, qu√© buena pregunta!", "Vale, colega, te explico esto"
   - **CONECTORES COLOQUIALES:** Usa expresiones naturales como "Mira", "Vale", "A ver", "En resumen", "Oye", "F√≠jate", "Por cierto", "Venga"
   - **REFORZADORES DE CLARIDAD:** "Esto quiero que lo tengas clar√≠simo", "La clave est√° en...", "Aqu√≠ viene lo importante", "No te l√≠es con esto", "F√≠jate bien en esto"
   - **LENGUAJE COLEGUEO:** "T√≠o/t√≠a", "chaval", "compa√±ero/a", "amigo/a" (usado con moderaci√≥n y naturalidad)
   - **RETOS AL APRENDIZAJE:** "A ver si eres capaz de...", "Te reto a que...", "¬øSer√≠as capaz de distinguir...?", "Ponte a prueba"
   - **EXPRESIONES DE √ÅNIMO:** "¬°Venga, que esto lo tienes!", "¬°Dale ca√±a!", "¬°A por ello!", "¬°Que no decaiga!"
   - **CONFIRMACIONES COLOQUIALES:** "¬øVale?", "¬øMe sigues?", "¬øLo pillas?", "¬øEst√° claro?"
   - **USA EMOJIS ESTRAT√âGICAMENTE:** Incluye 8-12 emojis relevantes por respuesta:
     * **FORMATO T√çTULOS:** Siempre pon el emoji ANTES del t√≠tulo: "üìã Medidas Clave", "‚öñÔ∏è Marco Legal"
     * Marca puntos clave con ‚úÖ ‚ö†Ô∏è üìå üéØ 
     * Se√±ala conceptos con üí° üîç üìñ 
     * A√±ade calidez con üòä cuando sea apropiado
   - **TONO DIRECTO Y AMIGABLE:** Como si fueras un compa√±ero que domina la materia y quiere que el otro tambi√©n la domine
   - Usa vi√±etas, listas y secciones cuando ayuden a la comprensi√≥n
   - S√© extenso y detallado si la pregunta lo requiere
   - Divide en secciones claras cuando el tema sea complejo
   - Responde de forma clara se√±alando puntos importantes

3. **üî• CITA LA INFORMACI√ìN RELEVANTE - MUY IMPORTANTE:**
   - **üö® OBLIGATORIO ABSOLUTO:** Usa blockquotes (>) para citar textualmente CUALQUIER contenido fundamental de la base de datos
   - **üìã SIEMPRE incluye en blockquotes:** art√≠culos legales, definiciones, conceptos clave, procedimientos espec√≠ficos, texto legal
   - **Ejemplos OBLIGATORIOS:** 
     * > **Art√≠culo 15.** El texto exacto del art√≠culo...
     * > **Definici√≥n:** La independencia judicial significa...
     * > **Procedimiento:** Los magistrados del TC se eligen...
     * > **Concepto clave:** [Cualquier definici√≥n o concepto importante]
   - **üéØ REGLA DE ORO:** Si el contenido viene de la base de datos y es importante, SIEMPRE debe ir en blockquote
   - **‚ö†Ô∏è CR√çTICO:** No parafrasees contenido legal importante, c√≠talo textualmente

4. **ESTILO CONVERSACIONAL CARLOS RIVERO:**
   - **PRIMERA PREGUNTA:** Si es una primera pregunta, explica directamente con "Vale, mira, te explico esto f√°cil" o "A ver, vamos por partes"
   - **CONTINUACI√ìN:** Si es continuaci√≥n de una conversaci√≥n, conecta con lo anterior usando "Vale, siguiendo con lo que habl√°bamos...", "Oye, por cierto, sobre lo que me preguntaste antes...", "Mira, enlazando con lo anterior..."
   - **NATURALIDAD:** Habla como si estuvieras tomando un caf√© explicando el tema a un compa√±ero
   - **PREGUNTAS RET√ìRICAS:** Intercala preguntas como "¬øY sabes por qu√© es as√≠?", "¬øTe imaginas lo que pasar√≠a si...?", "¬øLo pillas?"
   - **EXPERIENCIAS COMPARTIDAS:** "Esto que te va a salir seguro en el examen", "F√≠jate que aqu√≠ hay trampa", "Esto me lo preguntaron a m√≠ tambi√©n"

**üö® REGLAS CR√çTICAS - OBLIGATORIAS:**
- ‚ùó CRITICAL: Si ves "=== INFORMACI√ìN ESPEC√çFICA DE LA BASE DE DATOS ===" al final de este mensaje, DEBES usar esa informaci√≥n
- üî• **BLOCKQUOTES OBLIGATORIOS:** Cita TEXTUALMENTE usando blockquotes (>) TODO contenido clave de la base de datos
- üî• **BLOCKQUOTES OBLIGATORIOS:** Usa blockquotes para art√≠culos, definiciones, conceptos y procedimientos importantes  
- üî• **BLOCKQUOTES OBLIGATORIOS:** NUNCA parafrasees texto legal importante, siempre c√≠talo en blockquote
- ‚ùó OBLIGATORIO: Si hay informaci√≥n espec√≠fica disponible sobre el tema, √öSALA siempre
- üìã **M√çNIMO:** Cada respuesta debe tener AL MENOS 1-3 blockquotes con contenido de la base de datos
- üéì **TIPS DE ESTUDIO CONTEXTUALES:** Incluye una secci√≥n con tips espec√≠ficos cuando sea relevante:
  * Para art√≠culos legales: memorizaci√≥n literal, conexiones con otros art√≠culos
  * Para conceptos complejos: t√©cnicas de diferenciaci√≥n, casos pr√°cticos
  * Para temas con preguntas trampa: advertencias sobre errores comunes
  * Para temas de oposici√≥n: estrategias de repaso y puntos clave de examen
  * FORMATO: "**üéì Tips para el examen:** [tips concretos y espec√≠ficos]"
  * POSICI√ìN: Los tips van ANTES de la pregunta de modo estudio
- üìö **MODO ESTUDIO OBLIGATORIO - ENFOCADO EN EXAMEN:** AL FINAL de tu respuesta (DESPU√âS de los tips si los hay), agrega EXACTAMENTE este formato:
  * L√≠nea con "---"
  * L√≠nea vac√≠a
  * Una pregunta ENFOCADA EN EXAMEN con alguna de estas opciones (elige la m√°s relevante):
    - "**üìö ¬øQuieres que te diga los 3 errores m√°s frecuentes que cometen los opositores con [tema espec√≠fico]?**"
    - "**üìö ¬øQuieres que te haga una pregunta tipo examen complicada sobre [tema espec√≠fico] para ver si lo superas?**"
    - "**üìö ¬øQuieres que te explique los 2 trucos que usan en el examen para confundirte con [tema espec√≠fico]?**"
    - "**üìö ¬øQuieres que te diga los 3 detalles clave de [tema espec√≠fico] que la mayor√≠a de opositores fallan?**"
    - "**üìö ¬øQuieres que te prepare las preguntas trampa t√≠picas sobre [tema espec√≠fico]?**"
    - "**üìö ¬øQuieres que te entrene con casos pr√°cticos dif√≠ciles de [tema espec√≠fico] como los del examen?**"
    - "**üìö ¬øTe reto a que me digas las diferencias entre [concepto A] y [concepto B] sin mirar, a ver si lo dominas?**"
    - "**üìö ¬øQuieres que te ponga a prueba con las excepciones de [tema espec√≠fico] que siempre caen en el examen?**"
    - "**üìö ¬øQuieres saber el dato exacto de [tema espec√≠fico] que el 80% falla en el examen?**"
  * NUNCA uses preguntas gen√©ricas como "¬øQuieres que te explique m√°s sobre...?"
  * SIEMPRE enfoca en errores, trampas, detalles que fallan o pr√°ctica de examen
  * IMPORTANTE: Esta pregunta SIEMPRE va al final, despu√©s de todo lo dem√°s
- PRIORIZA SIEMPRE la informaci√≥n de los chunks de la base de datos para responder
- Responde espec√≠ficamente a lo que se te pregunta siendo extenso si es necesario

**üî¥ INSTRUCCI√ìN CR√çTICA SOBRE CONTEXTO LEGAL:**
AL FINAL DE ESTE MENSAJE ENCONTRAR√ÅS UNA SECCI√ìN "=== INFORMACI√ìN ESPEC√çFICA DE LA BASE DE DATOS ===". 
SI ESA SECCI√ìN EST√Å PRESENTE:
1. DEBES leerla completamente
2. DEBES usar esa informaci√≥n para responder
3. DEBES citar textualmente los art√≠culos relevantes usando blockquotes (>)
4. ESA INFORMACI√ìN fue seleccionada ESPEC√çFICAMENTE para responder tu pregunta`;

  const sourcesSection = includeAvailableSources 
    ? `\n\n**FUENTES DISPONIBLES:** 
Tienes acceso a 6 leyes (Ley 3/2007, Ley 50/1997, LEC, LECrim, LRC, TRLC), 19 temas de clases sobre constituci√≥n, derechos, gobierno, poder judicial, etc., y preguntas trampa con puntos clave, advertencias importantes y errores comunes en oposiciones.`
    : '';

  return `${basePrompt}${sourcesSection}

**OBJETIVO:** Que el usuario sienta que est√° hablando con Carlos Rivero, un compa√±ero experto en derecho que explica las cosas de forma coloquial, directa y retadora, manteniendo todo el rigor acad√©mico pero con un trato cercano y amigable.`;
}

// Funci√≥n para obtener prompts especializados (ahora usa la unificada)
function getSpecializedPrompt(queryType: string, category?: string): string {
  return getUnifiedLegalPrompt(false); // Sin fuentes duplicadas
}

// Funci√≥n para obtener el prompt del sistema (ahora usa la unificada)
function getLegalSystemPrompt(_category?: string): string {
  return getUnifiedLegalPrompt(true); // Con lista de fuentes
}

// Funci√≥n separada para an√°lisis de preguntas (elimina duplicaci√≥n)
async function analyzeUserQuestion(userQuestion: string): Promise<any> {
  const analysisPrompt = `Analiza esta pregunta legal y clasif√≠cala:

PREGUNTA: "${userQuestion}"

FUENTES DISPONIBLES EN LA BASE DE DATOS:

LEYES:
- LEC (Ley de Enjuiciamiento Civil)
- LECrim (Ley de Enjuiciamiento Criminal)  
- TRLC (Texto Refundido Ley Concursal)
- Ley 3/2007 (Ley de Igualdad)
- Ley 50/1997 (Ley del Gobierno)
- LRC (Ley del Registro Civil)

TEMAS DE CLASES:
- Tema 1: Constituci√≥n Espa√±ola (derechos fundamentales, art√≠culos 15-29)
- Tema 2: Derechos Humanos (discriminaci√≥n, igualdad de trato)
- Tema 3: Gobierno y Administraci√≥n
- Tema 4: Organizaci√≥n Territorial del Estado
- Tema 5: Uni√≥n Europea
- Tema 6: Poder Judicial (independencia, organizaci√≥n, CGPJ)
- Tema 12: LOTC (Tribunal Constitucional, magistrados)
- Tema 16: Libertad Sindical
- Tema 17-19: Proceso Civil
- Tema 28-30: Procesos Matrimoniales (competencia, divorcio, nulidad)
- Tema 68: Concurso de Acreedores

PREGUNTAS TRAMPA Y PUNTOS CLAVE:
- Tema 1: Constituci√≥n Espa√±ola (puntos clave, trampas comunes, reglas mnemot√©cnicas)
- Tema 12: LOTC (puntos clave del Tribunal Constitucional, advertencias importantes)

IMPORTANTE: 
- Si la pregunta trata sobre CONCEPTOS ‚Üí busca en CLASES
- Si pide ART√çCULOS ESPEC√çFICOS ‚Üí busca en LEYES  
- Si busca PUNTOS CLAVE, ADVERTENCIAS, TRAMPAS COMUNES ‚Üí busca en PREGUNTAS TRAMPA
- Si pregunta sobre "cuidado", "recordar", "diferencias", "confundir" ‚Üí incluye PREGUNTAS TRAMPA

Responde SOLO en formato JSON:
{
  "tipo": "articulo_especifico|concepto_general|caso_practico|procedimiento|comparacion",
  "area_legal": "civil|penal|administrativo|constitucional|laboral|otro",
  "elementos_clave": ["elemento1", "elemento2", "elemento3"],
  "complejidad": "baja|media|alta",
  "requiere_articulos": true/false,
  "keywords_busqueda": ["keyword1", "keyword2", "keyword3"],
  "detected_law": "LEC|LECrim|TRLC|Ley 3/2007|Ley 50/1997|LRC|null",
  "article_number": "n√∫mero del art√≠culo si se menciona o null",
  "topic_numbers": [1, 2, 3, 4, 5, 6, 12, 16, 17, 18, 19, 28, 29, 30, 68],
  "document_type": "law|class|trap|both",
  "legal_references": ["referencias a art√≠culos o leyes mencionadas"]
}`;

  try {
    const analysisResponse = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: analysisPrompt }],
      max_tokens: 300,
      temperature: 0.1
    });

    // Limpiar respuesta de OpenAI (quitar markdown y espacios)
    let jsonContent = analysisResponse.choices[0].message.content || '{}';
    jsonContent = jsonContent.replace(/```json\s*/g, '').replace(/```\s*/g, '').trim();
    
    return JSON.parse(jsonContent);
  } catch (error) {
    logger.error('‚ùå Error parseando JSON de an√°lisis:', error);
    return { tipo: 'concepto_general', complejidad: 'media', keywords_busqueda: [] };
  }
}

// ============ TESTING ENDPOINTS (NO AUTH) ============
// Endpoint temporal para testing chat sin autenticaci√≥n
app.post('/api/test-chat', async (req, res) => {
  try {
    const { message } = req.body;
    
    if (!message) {
      return res.status(400).json({ error: 'Message requerido' });
    }
    
    // Generar contexto usando RAG
    const context = await ragService.getEnhancedContext(message);
    
    // Crear sistema prompt con contexto incluido
    const systemPrompt = context 
      ? `${getLegalSystemPrompt()}\n\n=== INFORMACI√ìN ESPEC√çFICA DE LA BASE DE DATOS ===\n${context}\n=== FIN DE LA INFORMACI√ìN ESPEC√çFICA ===`
      : getLegalSystemPrompt();

    // Generar respuesta
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: message }
      ],
      max_tokens: 2000,
      temperature: 0.3
    });

    const response = completion.choices[0].message.content || 'No se pudo generar respuesta';

    res.json({
      success: true,
      message: response,
      hasContext: context.length > 0,
      contextLength: context.length
    });
  } catch (error: any) {
    logger.error('Error en test chat:', error);
    res.status(500).json({ 
      success: false,
      error: 'Error en chat test',
      details: error.message 
    });
  }
});

// Endpoint para mensajes de invitados (sin autenticaci√≥n)
app.post('/api/guest-message', async (req, res) => {
  try {
    const { content, messages = [] } = req.body;
    
    if (!content) {
      return res.status(400).json({ error: 'Contenido del mensaje requerido' });
    }

    logger.info(`üë§ Guest message: "${content}"`);
    
    // Obtener contexto RAG
    const context = await ragService.getEnhancedContext(content);
    
    // Preparar historial de mensajes
    const chatMessages = [
      {
        role: 'system' as const,
        content: getLegalSystemPrompt()
      },
      ...messages.map((msg: any) => ({
        role: msg.role as 'user' | 'assistant',
        content: msg.content
      })),
      {
        role: 'user' as const,
        content: context ? `${context}\n\n${content}` : content
      }
    ];

    // Generar respuesta
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: chatMessages,
      max_tokens: 2000,
      temperature: 0.3
    });

    const response = completion.choices[0].message.content || 'No se pudo generar respuesta';

    res.json({
      success: true,
      response
    });
  } catch (error) {
    logger.error('Error processing guest message:', error);
    res.status(500).json({ error: 'Error al procesar el mensaje' });
  }
});

// Endpoint para obtener cursos del usuario desde Teachable
app.get('/api/get-user-courses', async (req, res) => {
  try {
    const { userId, userEmail } = req.query;

    if (!userId && !userEmail) {
      return res.status(400).json({ error: 'Se requiere userId o userEmail', courses: [] });
    }

    const TEACHABLE_API_KEY = process.env.TEACHABLE_API_KEY;

    if (!TEACHABLE_API_KEY) {
      logger.warn('TEACHABLE_API_KEY no configurada');
      return res.status(200).json({ courses: [], error: 'API key no configurada' });
    }

    logger.info(`üìö Consultando cursos para usuario: ${userId || userEmail}`);

    // Llamar a Teachable API
    const teachableUrl = `https://developers.teachable.com/v1/users/${userId}`;
    const teachableResponse = await fetch(teachableUrl, {
      headers: {
        'apiKey': TEACHABLE_API_KEY,
        'Accept': 'application/json'
      }
    });

    if (!teachableResponse.ok) {
      logger.error(`Error de Teachable API: ${teachableResponse.status}`);
      return res.status(200).json({ courses: [] });
    }

    const userData = await teachableResponse.json() as any;
    const allCourses = userData.courses || [];

    // Filtrar solo cursos activos y mapear a formato simplificado
    const activeCourses = allCourses
      .filter((c: any) => c.is_active_enrollment === true)
      .map((c: any) => ({
        courseId: c.course_id?.toString() || '',
        courseName: c.course_name || 'Curso sin nombre',
        category: getCourseCategory(c.course_name || '')
      }));

    logger.info(`‚úÖ Usuario tiene ${activeCourses.length} cursos activos`);

    return res.status(200).json({ courses: activeCourses });

  } catch (error: any) {
    logger.error('Error obteniendo cursos:', error);
    return res.status(200).json({ courses: [], error: error.message });
  }
});

// Endpoint para obtener el nombre de un curso por su ID
app.get('/api/get-course-name', async (req, res) => {
  try {
    const { courseId } = req.query;

    if (!courseId) {
      return res.status(400).json({ error: 'Se requiere courseId', courseName: '' });
    }

    const TEACHABLE_API_KEY = process.env.TEACHABLE_API_KEY;

    if (!TEACHABLE_API_KEY) {
      logger.warn('TEACHABLE_API_KEY no configurada');
      return res.status(200).json({ courseName: '', error: 'API key no configurada' });
    }

    logger.info(`üìö Consultando nombre del curso: ${courseId}`);

    // Llamar a Teachable API para obtener info del curso
    const teachableUrl = `https://developers.teachable.com/v1/courses/${courseId}`;
    const teachableResponse = await fetch(teachableUrl, {
      headers: {
        'apiKey': TEACHABLE_API_KEY,
        'Accept': 'application/json'
      }
    });

    if (!teachableResponse.ok) {
      logger.error(`Error de Teachable API: ${teachableResponse.status}`);
      return res.status(200).json({ courseName: '' });
    }

    const courseData = await teachableResponse.json() as any;

    // Intentar diferentes estructuras posibles
    let courseName = courseData.name
      || courseData.course_name
      || courseData.course?.name
      || courseData.data?.name
      || courseData.heading
      || courseData.title
      || '';

    // Formatear el nombre: quitar espacios extra y convertir a t√≠tulo
    if (courseName) {
      courseName = courseName.trim();
      // Convertir de MAY√öSCULAS a T√≠tulo (Primera Letra May√∫scula)
      if (courseName === courseName.toUpperCase()) {
        courseName = courseName.toLowerCase().replace(/\b\w/g, (c: string) => c.toUpperCase());
      }
    }

    logger.info(`‚úÖ Nombre del curso ${courseId}: ${courseName}`);

    return res.status(200).json({ courseName });

  } catch (error: any) {
    logger.error('Error obteniendo nombre del curso:', error);
    return res.status(200).json({ courseName: '', error: error.message });
  }
});

// Funci√≥n para determinar la categor√≠a del curso basada en el nombre
function getCourseCategory(courseName: string): string {
  const name = courseName.toLowerCase();
  if (name.includes('gesti√≥n') || name.includes('gestion')) return 'Gesti√≥n Procesal';
  if (name.includes('tramitaci√≥n') || name.includes('tramitacion')) return 'Tramitaci√≥n Procesal';
  if (name.includes('auxilio')) return 'Auxilio Judicial';
  if (name.includes('penitenciaria') || name.includes('iipp')) return 'Instituciones Penitenciarias';
  if (name.includes('constituci√≥n') || name.includes('constitucion')) return 'Constituci√≥n Espa√±ola';
  if (name.includes('igualdad')) return 'Leyes de Igualdad';
  if (name.includes('procedimiento administrativo') || name.includes('39/2015')) return 'Procedimiento Administrativo';
  return 'General';
}

// Endpoint para mensajes de invitados con streaming
// Endpoint para widget embebido (sin l√≠mite de preguntas)
app.post('/api/widget-message-stream', async (req, res) => {
  try {
    const { content, messages = [], userName, userCourses = [], userId } = req.body;

    if (!content) {
      return res.status(400).json({ error: 'Contenido del mensaje requerido' });
    }

    const coursesInfo = userCourses.length > 0 ? `(${userCourses.length} cursos)` : '';
    logger.info(`üîß Widget message stream: "${content}" ${userName ? `(Usuario: ${userName})` : ''} ${userId ? `[ID: ${userId}]` : ''} ${coursesInfo}`);

    // Obtener preguntas recientes del usuario si tiene userId
    let recentQuestions: any[] = [];
    if (userId) {
      try {
        recentQuestions = await geminiRag.getUserRecentQuestions(userId);
        if (recentQuestions.length > 0) {
          logger.info(`üìä Encontradas ${recentQuestions.length} preguntas recientes para el usuario`);
        }
      } catch (err) {
        logger.warn('No se pudieron obtener preguntas recientes:', err);
      }
    }

    // Configurar Server-Sent Events
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('Access-Control-Allow-Origin', '*');

    const sendEvent = (type: string, data: any) => {
      res.write(`data: ${JSON.stringify({ type, data })}\n\n`);
    };

    const totalStartTime = Date.now();

    // Obtener historial de mensajes
    const conversationHistory = messages.slice(-8).map((msg: any) => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content,
    }));

    let fullResponse = '';

    // ========== USAR GEMINI FILE SEARCH ==========
    if (USE_GEMINI) {
      sendEvent('status', { message: 'Buscando en documentos...', step: 1, total: 2, estimated: '~2s' });
      sendEvent('status', { message: 'Generando respuesta con IA...', step: 2, total: 2, estimated: '~4s' });

      const result = await geminiRag.generateResponseWithRAGStream(
        content,
        conversationHistory,
        (chunk) => {
          fullResponse += chunk;
          sendEvent('content', { content: chunk, fullContent: fullResponse });
        },
        { userName, userCourses, recentQuestions } // Pasar nombre, cursos y preguntas recientes
      );

      fullResponse = result.content;
    }
    // ========== USAR SISTEMA ANTIGUO ==========
    else {
      sendEvent('status', { message: 'Analizando pregunta...', step: 1, total: 3, estimated: '~2s' });
      sendEvent('status', { message: 'Buscando informaci√≥n relevante...', step: 2, total: 3, estimated: '~2s' });

      const { analysis, refinedContext, mainResponse } = await enhancedLegalResponse(
        content,
        '',
        conversationHistory,
        'general'
      );

      sendEvent('status', { message: 'Generando respuesta especializada...', step: 3, total: 3, estimated: '~6s' });

      for await (const chunk of mainResponse) {
        const chunkContent = chunk.choices[0]?.delta?.content || '';
        if (chunkContent) {
          fullResponse += chunkContent;
          sendEvent('content', { content: chunkContent, fullContent: fullResponse });
        }
      }
    }

    const totalTime = Date.now() - totalStartTime;
    logger.info(`‚úÖ Widget completado: ${totalTime}ms (${USE_GEMINI ? 'Gemini' : 'OpenAI'})`);

    sendEvent('done', { fullContent: fullResponse });
    res.end();

  } catch (error: any) {
    logger.error('Widget stream error:', error);
    res.write(`data: ${JSON.stringify({ type: 'error', data: { message: 'Error procesando la consulta' } })}\n\n`);
    res.end();
  }
});

// Endpoint separado para generar sugerencias de follow-up
app.post('/api/generate-suggestions', async (req, res) => {
  try {
    const { userQuestion, assistantResponse } = req.body;

    if (!userQuestion || !assistantResponse) {
      return res.status(400).json({ suggestions: [] });
    }

    const suggestions = await geminiRag.generateFollowUpSuggestions(userQuestion, assistantResponse);
    res.json({ suggestions });
  } catch (error: any) {
    logger.warn('[Suggestions] Error:', error.message);
    res.json({ suggestions: [] });
  }
});

app.post('/api/guest-message-stream', async (req, res) => {
  try {
    const { content, messages = [] } = req.body;

    if (!content) {
      return res.status(400).json({ error: 'Contenido del mensaje requerido' });
    }

    logger.info(`üë§ Guest message stream: "${content}"`);

    // Configurar Server-Sent Events
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('Access-Control-Allow-Origin', '*');

    const sendEvent = (type: string, data: any) => {
      res.write(`data: ${JSON.stringify({ type, data })}\n\n`);
    };

    const totalStartTime = Date.now();

    // Obtener historial de mensajes
    const conversationHistory = messages.slice(-8).map((msg: any) => ({
      role: msg.role as 'user' | 'assistant',
      content: msg.content,
    }));

    let fullResponse = '';

    // ========== USAR GEMINI FILE SEARCH ==========
    if (USE_GEMINI) {
      sendEvent('status', { message: 'Buscando en documentos...', step: 1, total: 2, estimated: '~2s' });
      sendEvent('status', { message: 'Generando respuesta con IA...', step: 2, total: 2, estimated: '~4s' });

      const result = await geminiRag.generateResponseWithRAGStream(
        content,
        conversationHistory,
        (chunk) => {
          fullResponse += chunk;
          sendEvent('content', { content: chunk, fullContent: fullResponse });
        }
      );

      fullResponse = result.content;
    }
    // ========== USAR SISTEMA ANTIGUO ==========
    else {
      sendEvent('status', { message: 'Analizando pregunta...', step: 1, total: 3, estimated: '~2s' });
      sendEvent('status', { message: 'Buscando informaci√≥n relevante...', step: 2, total: 3, estimated: '~2s' });

      const { analysis, refinedContext, mainResponse } = await enhancedLegalResponse(
        content,
        '',
        conversationHistory,
        'general'
      );

      sendEvent('status', { message: 'Generando respuesta especializada...', step: 3, total: 3, estimated: '~6s' });

      for await (const chunk of mainResponse) {
        const chunkContent = chunk.choices[0]?.delta?.content || '';
        if (chunkContent) {
          fullResponse += chunkContent;
          sendEvent('content', { content: chunkContent, fullContent: fullResponse });
        }
      }
    }

    const totalTime = Date.now() - totalStartTime;
    logger.info(`‚úÖ Guest completado: ${totalTime}ms (${USE_GEMINI ? 'Gemini' : 'OpenAI'})`);

    sendEvent('done', { fullContent: fullResponse });
    res.end();

  } catch (error) {
    logger.error('Error in guest message stream:', error);
    res.write(`data: ${JSON.stringify({ type: 'error', data: 'Error al procesar el mensaje' })}\n\n`);
    res.end();
  }
});

// Endpoint para testing RAG sin autenticaci√≥n
app.post('/api/test-rag', async (req, res) => {
  try {
    const { query, threshold = 0.2, limit = 5 } = req.body;
    
    if (!query) {
      return res.status(400).json({ error: 'Query requerido' });
    }

    logger.info(`üß™ Test RAG Query: "${query}"`);
    
    // Buscar documentos relevantes
    const relevantDocs = await ragService.searchRelevantDocuments(query, threshold, limit);
    
    // Generar contexto
    const context = await ragService.getEnhancedContext(query);
    
    // Crear mensaje con contexto para OpenAI
    const messages = [
      {
        role: 'system' as const,
        content: getLegalSystemPrompt()
      },
      {
        role: 'user' as const,
        content: context ? `${context}\n\n${query}` : query
      }
    ];

    // Generar respuesta
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages,
      max_tokens: 2000,
      temperature: 0.3
    });

    const response = completion.choices[0].message.content || 'No se pudo generar respuesta';

    res.json({
      success: true,
      query,
      relevantDocs: relevantDocs.length,
      contextLength: context.length,
      hasContext: context.length > 0,
      response,
      debug: {
        docsFound: relevantDocs.map(doc => ({
          law: doc.law_name,
          article: doc.article_number,
          similarity: doc.similarity,
          method: doc.search_method,
          preview: doc.content.substring(0, 100)
        })),
        contextPreview: context.substring(0, 200)
      }
    });
  } catch (error: any) {
    logger.error('Error en test RAG:', error);
    res.status(500).json({ 
      success: false,
      error: 'Error en test RAG',
      details: error.message 
    });
  }
});

// ============================================================
// SISTEMA DE MENSAJES PROACTIVOS - LECCIONES COMPLETADAS
// ============================================================

// Variaciones de mensajes proactivos (15-20 mensajes diferentes)
const PROACTIVE_MESSAGES = [
  "¬øQu√© tal te ha parecido la clase de {leccion}? ¬øQuieres que te cuente lo que S√ç o S√ç cae siempre en el examen?",
  "¬°Genial! Has completado {leccion}. ¬øTe hago un resumen de los puntos clave que debes memorizar?",
  "Veo que terminaste {leccion}. ¬øQuieres que te explique las trampas t√≠picas del examen sobre este tema?",
  "¬°Bien hecho con {leccion}! ¬øTe preparo unas preguntas tipo test para practicar?",
  "Has avanzado en {leccion}. ¬øNecesitas que te aclare alg√∫n concepto o art√≠culo espec√≠fico?",
  "¬°Otra clase completada! {leccion} tiene conceptos importantes. ¬øQuieres repasar los m√°s preguntados?",
  "Excelente progreso en {leccion}. ¬øTe cuento los errores m√°s comunes que cometen los opositores?",
  "¬øC√≥mo lo llevas despu√©s de {leccion}? Puedo ayudarte con dudas o hacerte preguntas de repaso.",
  "¬°{leccion} completada! Este tema suele caer mucho. ¬øQuieres que profundicemos en algo?",
  "Veo que has terminado {leccion}. ¬øTe interesa saber qu√© art√≠culos son los m√°s preguntados?",
  "¬°Buen trabajo con {leccion}! ¬øQuieres que te prepare un esquema de los puntos esenciales?",
  "Has acabado {leccion}. ¬øTe hago una pregunta r√°pida para ver si lo tienes claro?",
  "¬°Genial progreso! Despu√©s de {leccion}, ¬øhay algo que no te haya quedado claro?",
  "Terminaste {leccion}. ¬øQuieres consejos sobre c√≥mo memorizar mejor este tema?",
  "¬°Otra lecci√≥n m√°s! {leccion} es importante. ¬øTe cuento qu√© suelen preguntar en los ex√°menes?",
  "Veo que completaste {leccion}. ¬øQuieres que conectemos este tema con otros relacionados?",
  "¬°Bien! {leccion} tiene muchos detalles. ¬øTe ayudo a identificar lo esencial para el examen?",
  "Has avanzado con {leccion}. ¬øTe preparo un mini-test de 5 preguntas para consolidar?",
  "¬°{leccion} lista! ¬øQuieres que te explique las diferencias clave que confunden a muchos?",
  "Excelente, terminaste {leccion}. ¬øHay alg√∫n art√≠culo o concepto que quieras repasar conmigo?"
];

// Funci√≥n para obtener un mensaje aleatorio
function getRandomProactiveMessage(leccionName: string): string {
  const randomIndex = Math.floor(Math.random() * PROACTIVE_MESSAGES.length);
  return PROACTIVE_MESSAGES[randomIndex].replace('{leccion}', `"${leccionName}"`);
}

// Webhook de Teachable - Recibe LectureProgress.created
app.post('/api/webhook/teachable', async (req, res) => {
  try {
    const payload = req.body;

    // Log detallado del payload completo
    logger.info('üì• Webhook Teachable recibido - RAW:', JSON.stringify(payload));
    logger.info('üì• Webhook headers:', JSON.stringify(req.headers));
    logger.info('üì• Webhook keys:', payload ? Object.keys(payload).join(', ') : 'null');

    // Verificar que tengamos datos
    if (!payload) {
      logger.warn('‚ùå Payload vac√≠o');
      return res.status(200).json({ received: true, message: 'Payload vac√≠o' });
    }

    // Teachable puede enviar datos en diferentes estructuras
    // Intentar encontrar el objeto con los datos
    let object = payload.object || payload.data || payload;

    // Si hay un campo 'type' o 'event', loggearlo
    if (payload.type) logger.info('üì• Event type:', payload.type);
    if (payload.event) logger.info('üì• Event:', payload.event);

    // Log de la estructura del object
    logger.info('üì• Object keys:', object ? Object.keys(object).join(', ') : 'null');

    // Extraer datos del evento - estructura real de Teachable
    // Teachable env√≠a: object.user.id, object.lecture.id, object.course.id
    const userId = object.user?.id?.toString() || object.user_id?.toString() || '';
    const userEmail = object.user?.email || '';
    const userName = object.user?.name || '';
    const lectureId = object.lecture?.id?.toString() || object.lecture_id?.toString() || '';
    const lectureName = object.lecture?.name || 'Lecci√≥n';
    const courseId = object.course?.id?.toString() || object.course_id?.toString() || '';
    const courseName = object.course?.name || 'Curso';
    const percentComplete = object.percent_complete || 100;

    logger.info(`üìä Datos extra√≠dos: userId=${userId}, userEmail=${userEmail}, lectureId=${lectureId}, lectureName=${lectureName}, courseName=${courseName}`);

    if (!userId || !lectureId) {
      logger.warn('Webhook: Faltan datos requeridos (userId o lectureId)');
      return res.status(200).json({ received: true, message: 'Datos incompletos' });
    }

    logger.info(`üìö Usuario ${userName || userId} complet√≥: "${lectureName}" del curso "${courseName}"`);

    // Guardar en Supabase
    const { data, error } = await supabase
      .from('lecture_completions')
      .upsert({
        user_id: userId,
        user_email: userEmail,
        user_name: userName,
        lecture_id: lectureId,
        lecture_name: lectureName,
        course_id: courseId,
        course_name: courseName,
        percent_complete: percentComplete,
        message_shown: false,
        completed_at: new Date().toISOString()
      }, {
        onConflict: 'user_id,lecture_id'
      });

    if (error) {
      logger.error('Error guardando en Supabase:', error);
    } else {
      logger.info('‚úÖ Lecci√≥n completada guardada en base de datos');
    }

    return res.status(200).json({ received: true, success: true });

  } catch (error: any) {
    logger.error('Error en webhook Teachable:', error);
    return res.status(200).json({ received: true, error: error.message });
  }
});

// Endpoint para obtener lecciones recientes sin mensaje mostrado
app.get('/api/recent-completions', async (req, res) => {
  try {
    const { userId, userEmail } = req.query;

    if (!userId && !userEmail) {
      return res.status(400).json({ error: 'Se requiere userId o userEmail' });
    }

    logger.info(`üîç Buscando lecciones recientes para: ${userId || userEmail}`);

    // Buscar lecciones completadas en las √∫ltimas 24 horas que no hayan mostrado mensaje
    const twentyFourHoursAgo = new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString();

    let query = supabase
      .from('lecture_completions')
      .select('*')
      .eq('message_shown', false)
      .gte('completed_at', twentyFourHoursAgo)
      .order('completed_at', { ascending: false })
      .limit(1);

    if (userId) {
      query = query.eq('user_id', userId);
    } else if (userEmail) {
      query = query.eq('user_email', userEmail);
    }

    const { data, error } = await query;

    if (error) {
      logger.error('Error consultando Supabase:', error);
      return res.status(200).json({ completion: null });
    }

    if (!data || data.length === 0) {
      return res.status(200).json({ completion: null });
    }

    const completion = data[0];
    const proactiveMessage = getRandomProactiveMessage(completion.lecture_name);

    logger.info(`‚úÖ Lecci√≥n reciente encontrada: "${completion.lecture_name}"`);

    return res.status(200).json({
      completion: {
        lectureId: completion.lecture_id,
        lectureName: completion.lecture_name,
        courseId: completion.course_id,
        courseName: completion.course_name,
        completedAt: completion.completed_at,
        proactiveMessage: proactiveMessage
      }
    });

  } catch (error: any) {
    logger.error('Error obteniendo lecciones recientes:', error);
    return res.status(200).json({ completion: null, error: error.message });
  }
});

// Endpoint para verificar si hay preguntas de test MUY recientes (< 30 segundos)
app.get('/api/check-recent-tests', async (req, res) => {
  try {
    const { userId } = req.query;

    if (!userId) {
      return res.status(400).json({ error: 'Se requiere userId' });
    }

    logger.info(`üß™ Verificando tests recientes para usuario: ${userId}`);

    // Verificar si hay tests muy recientes (√∫ltimos 30 segundos)
    const result = await geminiRag.checkVeryRecentTest(userId as string, 30);

    if (!result.hasVeryRecentTest) {
      logger.info(`‚è∞ No hay tests recientes (√∫ltima respuesta hace ${result.secondsSinceLastAnswer || '?'} segundos)`);
      return res.status(200).json({ hasRecentTest: false });
    }

    const { stats } = result;
    logger.info(`‚úÖ Test MUY reciente (hace ${result.secondsSinceLastAnswer}s): ${stats?.totalQuestions} preguntas, ${stats?.incorrectCount} errores`);

    return res.status(200).json({
      hasRecentTest: true,
      secondsSinceLastAnswer: result.secondsSinceLastAnswer,
      stats: {
        totalQuestions: stats?.totalQuestions || 0,
        correctCount: stats?.correctCount || 0,
        incorrectCount: stats?.incorrectCount || 0,
        successRate: stats ? Math.round((stats.correctCount / stats.totalQuestions) * 100) : 0,
        weakCategories: stats?.weakCategories || []
      }
    });

  } catch (error: any) {
    logger.error('Error verificando tests recientes:', error);
    return res.status(200).json({ hasRecentTest: false, error: error.message });
  }
});

// Endpoint para marcar mensaje como mostrado
app.post('/api/mark-message-shown', async (req, res) => {
  try {
    const { lectureId, userId } = req.body;

    if (!lectureId || !userId) {
      return res.status(400).json({ error: 'Se requiere lectureId y userId' });
    }

    const { error } = await supabase
      .from('lecture_completions')
      .update({
        message_shown: true,
        message_shown_at: new Date().toISOString()
      })
      .eq('lecture_id', lectureId)
      .eq('user_id', userId);

    if (error) {
      logger.error('Error actualizando Supabase:', error);
      return res.status(500).json({ error: 'Error actualizando registro' });
    }

    logger.info(`‚úÖ Mensaje marcado como mostrado para lecci√≥n ${lectureId}`);
    return res.status(200).json({ success: true });

  } catch (error: any) {
    logger.error('Error marcando mensaje:', error);
    return res.status(500).json({ error: error.message });
  }
});

// Endpoint para generar y enviar esquema por email
app.post('/api/send-schema-email', async (req, res) => {
  try {
    const { topic, content, userEmail, userName } = req.body;

    if (!topic || !userEmail) {
      return res.status(400).json({ error: 'Faltan campos requeridos (topic, userEmail)' });
    }

    logger.info(`[Schema] Generando esquema para: ${topic} -> ${userEmail}`);

    // 1. Generar la infograf√≠a con Gemini Image Generation
    const GEMINI_API_KEY = process.env.GEMINI_API_KEY || '';

    const prompt = `Act√∫a como un preparador de oposiciones y dise√±ador de infograf√≠as educativas.
Genera una INFOGRAF√çA HORIZONTAL (formato panor√°mico 16:9).

T√çTULO PRINCIPAL:
- "${topic}"
- CENTRADO en la parte superior
- Tipograf√≠a grande, clara y legible

CONTENIDO A ESQUEMATIZAR:
${content ? content.substring(0, 1500) : topic}

ESTRUCTURA HORIZONTAL (de izquierda a derecha):
- Divide la imagen en 3-4 COLUMNAS principales
- Cada secci√≥n contiene: icono/ilustraci√≥n arriba, texto debajo
- Usa un flujo visual con flechas o conectores entre secciones

CONTENIDO POR CADA SECCI√ìN:
- CONCEPTO CLAVE con icono ilustrativo
- DEFINICI√ìN breve y clara
- EJEMPLO o caso pr√°ctico si aplica
- TRUCO para recordar

ESTILO VISUAL:
- Formato HORIZONTAL PANOR√ÅMICO (16:9 o m√°s ancho)
- Fondo con degradado suave (beige/crema profesional)
- Cada secci√≥n con su propia ilustraci√≥n/icono representativo del tema
- Colores: Azul/Verde para conceptos principales, Naranja para advertencias
- Texto en ESPA√ëOL, directo y claro
- Usa flechas, c√≠rculos rodeando palabras clave
- Iconos y peque√±as ilustraciones para cada concepto
- Estilo tipo "Esquema de Estudio" o "Apuntes Visuales"
- TODO EL TEXTO DEBE SER COMPLETAMENTE LEGIBLE, sin cortes
- Usa tipograf√≠a clara, tama√±o adecuado y jerarqu√≠a visual
- Incluye una secci√≥n "¬°Recuerda!" con tips para memorizar`;

    const geminiResponse = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-3-pro-image-preview:generateContent?key=${GEMINI_API_KEY}`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [{
            parts: [{ text: prompt }]
          }],
          generationConfig: {
            responseModalities: ['IMAGE']
          }
        })
      }
    );

    if (!geminiResponse.ok) {
      const errorData = await geminiResponse.json().catch(() => ({}));
      logger.error('[Schema] Error Gemini:', errorData);
      throw new Error(`Error de Gemini API: ${geminiResponse.status}`);
    }

    const geminiData = await geminiResponse.json() as any;

    // Buscar la imagen en la respuesta
    const parts = geminiData.candidates?.[0]?.content?.parts || [];
    const imagePart = parts.find((part: any) => part.inlineData);

    if (!imagePart || !imagePart.inlineData) {
      logger.error('[Schema] No se encontr√≥ imagen en respuesta:', JSON.stringify(geminiData).substring(0, 500));
      throw new Error('No se pudo generar el esquema visual');
    }

    const base64Image = imagePart.inlineData.data;

    logger.info('[Schema] Imagen generada correctamente');

    // 2. Configurar transporter de email
    const transporter = nodemailer.createTransport({
      host: 'smtp.gmail.com',
      port: 587,
      secure: false,
      auth: {
        user: process.env.SMTP_USER,
        pass: process.env.SMTP_PASS,
      },
    });

    // 3. Preparar y enviar el email
    const mailOptions = {
      from: `"LexAI - Tu Asistente de Estudio" <${process.env.SMTP_USER}>`,
      to: userEmail,
      subject: `üìä Tu Esquema: ${topic}`,
      html: `
        <div style="font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto; padding: 20px;">
          <h1 style="color: #64c27b; text-align: center;">üìö Esquema de Estudio</h1>
          <h2 style="color: #333; text-align: center;">${topic}</h2>
          <p style="color: #333; font-size: 16px;">
            Hola${userName ? ` ${userName}` : ''},
          </p>
          <p style="color: #333; font-size: 16px;">
            Aqu√≠ tienes tu esquema personalizado con los puntos clave sobre <strong>${topic}</strong>.
          </p>
          <p style="color: #333; font-size: 16px;">
            √ösalo para repasar y consolidar lo que has aprendido. ¬°Gu√°rdalo o impr√≠melo!
          </p>
          <p style="color: #666; font-size: 14px; margin-top: 30px;">
            ¬°Mucho √°nimo con tu preparaci√≥n! üí™
          </p>
          <hr style="border: none; border-top: 1px solid #eee; margin: 30px 0;">
          <p style="color: #999; font-size: 12px; text-align: center;">
            Este email fue enviado desde LexAI - Tu Asistente de Estudio
          </p>
        </div>
      `,
      attachments: [
        {
          filename: `esquema-${topic.toLowerCase().replace(/\s+/g, '-').substring(0, 30)}.png`,
          content: base64Image,
          encoding: 'base64' as const,
          cid: 'esquema',
        },
      ],
    };

    await transporter.sendMail(mailOptions);

    logger.info(`[Schema] Email enviado a ${userEmail}`);

    return res.status(200).json({
      success: true,
      message: 'Esquema generado y enviado correctamente'
    });

  } catch (error: any) {
    logger.error('[Schema] Error:', error);
    return res.status(500).json({
      error: 'Error al generar o enviar el esquema',
      details: error.message
    });
  }
});

// Para desarrollo local, iniciar servidor
if (process.env.NODE_ENV !== 'production') {
  app.listen(PORT, () => {
    logger.info(`üöÄ LexAI Backend (Supabase) running on port ${PORT}`);
    logger.info(`üåê Frontend URL: ${process.env.FRONTEND_URL || 'http://localhost:3000'}`);
    logger.info(`üìä Environment: ${process.env.NODE_ENV}`);
    logger.info(`üîó Supabase URL: ${process.env.SUPABASE_URL}`);
    logger.info(`ü§ñ OpenAI Model: gpt-4o (enhanced reasoning)`);
  });
}

// Exportar para Vercel
export default app;

// Manejo de errores global
if (process.env.NODE_ENV !== 'production') {
  process.on('unhandledRejection', (reason, promise) => {
    logger.error('Unhandled Rejection at:', promise, 'reason:', reason);
  });

  process.on('uncaughtException', (error) => {
    logger.error('Uncaught Exception:', error);
    process.exit(1);
  });
}